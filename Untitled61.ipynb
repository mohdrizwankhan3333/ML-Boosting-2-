{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc651e1-d1a0-476b-8e50-87ae3ea69608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.386\n",
      "R-squared: -0.000\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Best Score: -1.410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nQ4. What is a weak learner in Gradient Boosting?\\nA weak learner is a simple model (e.g., a shallow decision tree) that performs slightly better than random guessing but not \\nvery accurate on its own.\\n\\nQ5. What is the intuition behind the Gradient Boosting algorithm?\\nGradient Boosting builds an ensemble by iteratively adding models that correct the errors (residuals) of previous models using gradient descent.\\n\\nQ6. How does Gradient Boosting algorithm build an ensemble of weak learners?\\nGradient Boosting sequentially trains weak learners, each focusing on the residual errors of the combined ensemble from previous \\niterations to improve overall performance.\\n\\nQ7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\\n\\n1. Initialize: the model with a simple predictor (e.g., mean of the target).\\n2. Compute Residuals: Calculate the difference between actual and predicted values.\\n3. Fit Weak Learner: Train a weak learner to predict the residuals.\\n4. Update Model: Add the weak learner's prediction to the ensemble model.\\n5. Iterate: Repeat steps 2-4 for a specified number of iterations or until convergence.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is an ensemble technique that builds models sequentially to minimize the residual errors using gradient \n",
    "descent, improving accuracy.\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example \n",
    "and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.**\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Simple dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 3.7, 3.2, 4.8, 6.1])\n",
    "\n",
    "# Initialize parameters\n",
    "learning_rate = 0.1\n",
    "n_estimators = 100\n",
    "residuals = y.copy()\n",
    "predictions = np.zeros_like(y)\n",
    "\n",
    "for _ in range(n_estimators):\n",
    "        #                 Fit a simple model (mean predictor)\n",
    "    prediction = residuals.mean()\n",
    "    predictions += learning_rate * prediction\n",
    "    \n",
    "                                                             # Calculate residuals\n",
    "    residuals = y - predictions\n",
    "\n",
    "           # Evaluate performance\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.3f}')\n",
    "print(f'R-squared: {r2:.3f}')\n",
    "'''\n",
    "\n",
    "**Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model.\n",
    "Use grid search or random search to find the best hyperparameters.**\n",
    "\n",
    "Use `GridSearchCV` from sklearn for hyperparameter tuning:\n",
    "\n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define parameter grid'\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Model and grid search\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best Score: {grid_search.best_score_:.3f}')\n",
    "'''\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "A weak learner is a simple model (e.g., a shallow decision tree) that performs slightly better than random guessing but not \n",
    "very accurate on its own.\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Gradient Boosting builds an ensemble by iteratively adding models that correct the errors (residuals) of previous models using gradient descent.\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Gradient Boosting sequentially trains weak learners, each focusing on the residual errors of the combined ensemble from previous \n",
    "iterations to improve overall performance.\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "1. Initialize: the model with a simple predictor (e.g., mean of the target).\n",
    "2. Compute Residuals: Calculate the difference between actual and predicted values.\n",
    "3. Fit Weak Learner: Train a weak learner to predict the residuals.\n",
    "4. Update Model: Add the weak learner's prediction to the ensemble model.\n",
    "5. Iterate: Repeat steps 2-4 for a specified number of iterations or until convergence.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
